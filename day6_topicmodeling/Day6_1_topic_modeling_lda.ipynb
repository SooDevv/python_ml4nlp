{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터라이징을 한 뒤 term frequency matrix까지 만드는 부분은 빠르게 넘어가도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_fname = '../../../data/corpus_10days/news/2016-10-24_article_all_normed.txt'\n",
    "mm_fname = '../../../data/corpus_10days/models/2016-10-24_article_all_normed_corpus.mtx'\n",
    "mm_vocab = '../../../data/corpus_10days/models/2016-10-24_article_all_normed_corpus.vocab'\n",
    "dictionary_fname = '../../../data/corpus_10days/models/2016-10-24_article_all_normed_corpus.dictionary'\n",
    "\n",
    "ldamodel_fname = '../../../data/corpus_10days/models/2016-10-24_article_all_normed_corpus_lda_3_6.pkl'\n",
    "\n",
    "TRAIN_LDA = False\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.io import mmwrite, mmread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num docs = 26368\n",
      "(26368, 4760)\n"
     ]
    }
   ],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "corpus = DoublespaceLineCorpus(corpus_fname, iter_sent=False)\n",
    "print('num docs =', len(corpus))\n",
    "\n",
    "\n",
    "with open('2016-10-24-extract_noun_dict.pkl', 'rb') as f:\n",
    "    noun_dict = pickle.load(f)\n",
    "    \n",
    "def custom_tokenize(doc):    \n",
    "    def parse_noun(token):\n",
    "        for e in reversed(range(1, len(token)+1)):\n",
    "            subword = token[:e]\n",
    "            if subword in noun_dict:\n",
    "                return subword\n",
    "        return ''\n",
    "    \n",
    "    nouns = [parse_noun(token) for token in doc.split()]\n",
    "    nouns = [word for word in nouns if word]\n",
    "    return nouns\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenize)\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "idx2vocab = [vocab for vocab, idx in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])]\n",
    "\n",
    "# mmwrite(mm_fname, x)\n",
    "# with open(mm_vocab, 'w', encoding='utf-8') as f:\n",
    "#     for word, _ in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1]):\n",
    "#         f.write('%s\\n' % word)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim tutorial\n",
    "\n",
    "### [Corpus Formats][corpus_format]\n",
    "\n",
    "Gensim의 LDA가 이용하는 학습데이터의 형식은 아래와 같습니다. list of list of tuple이며, tuple은 (term_index, frequency) 입니다.\n",
    "\n",
    "\n",
    "    from gensim import corpora\n",
    "    \n",
    "    # create a toy corpus of 2 documents, as a plain Python list\n",
    "    corpus = [[(1, 0.5)], []]\n",
    "    \n",
    "scikit-learn 을 이용하여 sparse matrix 를 미리 만들었다면 gensim.matutils.Sparse2Corpus 를 이용하여 이를 변형합니다. 그런데, gensim은 sparse matrix를 (doc, term) matrix가 아니라 (term, doc) matrix 라고 가정합니다. 그래서 Sparse2Corpus 의 documents_columns 를 False 로 지정해줍니다. Default 는 True 입니다.\n",
    "\n",
    "    from scipy.io import mmread\n",
    "    \n",
    "    scipy_sparse_matrix = mmread(mm_fname)    \n",
    "    # corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)\n",
    "    corpus = gensim.matutils.Sparse2Corpus(x, documents_columns=False)\n",
    "\n",
    "[corpus_format]: https://radimrehurek.com/gensim/tut1.html#corpus-formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#doc= 0:  [] ...\n",
      "#doc= 1:  [(4592, 1), (818, 2), (697, 1), (3202, 1), (4678, 1), (530, 3), (1188, 1), (1605, 1), (3152, 1), (2104, 1)] ...\n",
      "#doc= 2:  [(2552, 1), (4671, 1), (1174, 1), (3678, 1), (4615, 2), (447, 1), (4740, 1), (1871, 1), (3131, 1), (3679, 1)] ...\n",
      "#doc= 3:  [(3868, 1), (3513, 1), (2701, 1), (2117, 1), (756, 1), (2858, 1), (1890, 1), (2530, 1), (2189, 1), (469, 1)] ...\n"
     ]
    }
   ],
   "source": [
    "corpus = gensim.matutils.Sparse2Corpus(x, documents_columns=False)\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    if i > 3: break\n",
    "    print('#doc= %d: ' % i, doc[:10], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26368"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA 를 학습할 때에는 corpus 에 등장한 4592 번이 어떤 단어인지 정보를 알아야 합니다. 이를 위해 vectorizer 로부터 id2word 라는 list of str 을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = {\n",
    "    idx:vocab for vocab, idx in\n",
    "    sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터를 gensim LDA format으로 맞추는 일이 끝났습니다.\n",
    "\n",
    "num_topics는 토픽의 개수를 정하는 부분입니다. id2word를 입력하지 않으면 단어가 term index로 출력됩니다. Gensim 의 id2word 는 list of str 이 아닌, dict 구조여야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습시간 (i7-5820): 1min 15sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "if TRAIN_LDA:\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=50, id2word=id2word)\n",
    "    import pickle\n",
    "    with open(ldamodel_fname, 'wb') as f:\n",
    "        pickle.dump(ldamodel, f)\n",
    "else:\n",
    "    import pickle\n",
    "    with open(ldamodel_fname, 'rb') as f:\n",
    "        ldamodel = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print_topic은 특정 topic에 대하여 설명력이 좋은 (topic probability가 높은) topn개의 단어를 prob.와 함께 출력해줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.202*\"뉴스\" + 0.136*\"저작권자\" + 0.046*\"카페\" + 0.041*\"웹툰\" + 0.041*\"02\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topic(10, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_topic_terms(topic_id)를 하면 term index가 출력되기 때문에 print_topic()의 결과에서 단어만 선택하는 parse_topic_words() 함수를 만들어 50개의 토픽에 대하여 각각 대표단어를 뽑아봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2046, 0.030306516),\n",
       " (3660, 0.021786332),\n",
       " (2183, 0.021200346),\n",
       " (4364, 0.020792965),\n",
       " (1955, 0.01804878),\n",
       " (905, 0.017082699),\n",
       " (4664, 0.015296279),\n",
       " (1105, 0.014546601),\n",
       " (4453, 0.0141129615),\n",
       " (2255, 0.011956487)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_topic_terms(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#topic= 0: ['단지', '아파트', '이용', '예정', '해수', '분양', '지역', '환경', '위치', '골프']\n",
      "#topic= 1: ['사진', '제품', '선보', '패션', '브랜드', '기사', '화보', '다양', '피부', '세번째']\n",
      "#topic= 2: ['웃음', '클린턴', '있습니다', '트럼프', '미국', '혼술', '후보', '앵커', '방송', '라고']\n",
      "#topic= 3: ['기업', '기술', '지원', '사업', '세계', '창업', '개발', '글로벌', '산업', '한국']\n",
      "#topic= 4: ['영화', '공연', '일간스포츠', '개봉', '고양이', '작품', '할머니', '장애인', '관객', '배우']\n",
      "\n",
      "#topic= 5: ['개헌', '우리', '국민', '헌법', '사회', '대한민국', '대통령', '지금', '개정', '체제']\n",
      "#topic= 6: ['제보', '배포', '라이브', '기억', '동영상', '전주', '11월', '훈훈', '자신', '결정']\n",
      "#topic= 7: ['현대', '인천', '공장', '필리핀', '대사', '이동', '건조', '러시아', '옥녀', '계약']\n",
      "#topic= 8: ['예산', '정부', '대통령', '국회', '예산안', '지원', '편성', '경제', '시정연설', '관련']\n",
      "#topic= 9: ['행사', '지난', '참여', '제공', '000원', '진행', '함께', '현지', '부산', '전국']\n",
      "\n",
      "#topic= 10: ['뉴스', '저작권자', '카페', '웹툰', '02', '이용', '인터넷', '01', '24', '이용자']\n",
      "#topic= 11: ['문화', '체험', '지역', '제공', '진행', '여행', '운영', '할인', '마련', '다양']\n",
      "#topic= 12: ['총장', '청춘', '충남', '대학', '한국', '세종역', '신설', '한의사', '아시아', '세종']\n",
      "#topic= 13: ['남자', '여자', '감독', '수애', '왕소', '황제', '신인', '르노삼성', '영화', '고객']\n",
      "#topic= 14: ['24', '오후', '독도', '참석', '인도네시아', '제보', '전남', '연습', '포토타임', '전파']\n",
      "\n",
      "#topic= 15: ['공시', '결제', '카드', '신한', '조선', '계약', '카카오톡', '인턴', '미흡', '제품으']\n",
      "#topic= 16: ['뉴스1', '24', '마약', '라디오', '선고', '판결', '대표', '재판', '두테르테', '대통령']\n",
      "#topic= 17: ['치료', '환자', '경우', '건강', '사용', '여성', '이상', '연구', '발생', '병원']\n",
      "#topic= 18: ['24', '국회', '2016', '한남동', '여의도', '시정연설', '박근혜', '대통령', '예산안', '오전']\n",
      "#topic= 19: ['조사', '사실', '경찰', '것으', '혐의', '확인', '검찰', '사건', '수사', '청와대']\n",
      "\n",
      "#topic= 20: ['아시아경제', '인스타그램', '수상', '24', '기사', '선정', '2016', '오후', '경기', '세계']\n",
      "#topic= 21: ['대구', '제주', '요리', '아들', '제주도', '아내', '학생들', '바다', '서울시', '이별']\n",
      "#topic= 22: ['스포츠', '의혹', '난민', '재단', '최씨', '검찰', '독일', '것으', '한글', '설립']\n",
      "#topic= 23: ['무대', '사랑', '함께', '노래', '음악', '모습', '활동', '팬들', '마음', '가수']\n",
      "#topic= 24: ['데뷔', '배우', '연기', '드라마', '방송', '변신', '라고', '작품', '자신', '시청자들']\n",
      "\n",
      "#topic= 25: ['네이버', '삼성', '중국어선', '삼성전자', '조직', '기술', '선임', '독립', '이사회', '24']\n",
      "#topic= 26: ['한국일보', '경우', '이들', '서울신문', '경찰', '운영', '것으', '불법', '위자료', '지난해']\n",
      "#topic= 27: ['부산', '자전거', '뿐만', '파업', '보컬', '왕십리', '오후', '성동구', '임대', '2차']\n",
      "#topic= 28: ['지진', '교육', '경기도', '발생', '수원', '케미', '오전', '학교', '피해', '청소년']\n",
      "#topic= 29: ['사진', '영상', '진영', '잠시', '24', '터키', '표정', '박주', '본부', '연예인']\n",
      "\n",
      "#topic= 30: ['북한', '외교', '중국', '우리', '정부', '주장', '역도', '미국', '논란', '대북']\n",
      "#topic= 31: ['장관', '대표', '새누리당', '대통령', '회의', '회고록', '24', '더불어민주당', '관련', '문재인']\n",
      "#topic= 32: ['국내', '해외', '한국', '수출', '아이', '프로', '개발', '루이', '수산', '사용']\n",
      "#topic= 33: ['트와이스', '공개', '앨범', '걸그룹', '사랑', '24', '쇼케이스', '매력', '신곡', '미니앨범']\n",
      "#topic= 34: ['일본', '서비스', '온라인', '엑소', '모바일', '고객', '이벤트', '유로', '무릎', '세계']\n",
      "\n",
      "#topic= 35: ['2016', '24', '00', '참석', '23', '기사', '헤어질까', '기념', '오후', '박규리']\n",
      "#topic= 36: ['롯데', '회장', '방탄', '그룹', '눈빛', '후문', '나비', '상장', '한국', '지분']\n",
      "#topic= 37: ['가격', '부동산', '것으', '시장', '정부', '대출', '올해', '평균', '지난해', '기준']\n",
      "#topic= 38: ['개헌', '대통령', '국회', '헌법', '임기', '논의', '제안', '추진', '정부', '대선']\n",
      "#topic= 39: ['개헌', '대통령', '대표', '논의', '국민', '입장', '박근혜', '최순실', '국회', '제안']\n",
      "\n",
      "#topic= 40: ['게임', '라인', '화면', '청취', '블랙', '동물', '색상', '조준', '일들', '모니터']\n",
      "#topic= 41: ['상승', '주가', '투자', '거래', '매수', '기록', '하락', '24', '대비', '농협']\n",
      "#topic= 42: ['스타일', '코트', '헤드', '스포츠', '노선', '셰프', '경리', '직장인', '특급', '착용']\n",
      "#topic= 43: ['방송', '뉴미디어', '사진', '공개', '배우', '미디어', '구르미', '연설문', '출연', '자신']\n",
      "#topic= 44: ['고호', '방송', '예능', '사고', '모습', '23일', '유지', '복면가왕', '이날', '출연']\n",
      "\n",
      "#topic= 45: ['시장', '것으', '리포트', '판매', '매출', '지난해', '출시', '국내', '올해', '증가']\n",
      "#topic= 46: ['갤럭시', '삼성전자', '교환', '갤럭시노트7', '프로그램', '노트7', '사용', '지원', '미소', '출시']\n",
      "#topic= 47: ['가을', '경남', '저축', '금융', '24', '강원', '블락비', '안전', '기온', '강원도']\n",
      "#topic= 48: ['비정상', '미국', '증상', '궁금증', '유엔', '이목', '장식', '대화', '압박', '제재']\n",
      "#topic= 49: ['때문', '생각', '하지', '문제', '지금', '상황', '사람', '자신', '라고', '대표']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_topic_words(topic_str):\n",
    "    return [col.split('*\"')[1][:-1] for col in topic_str.split(' + ')]\n",
    "\n",
    "for i in range(50):\n",
    "    print('#topic= %d:' % i, parse_topic_words( ldamodel.print_topic(i, topn=10)) )\n",
    "    \n",
    "    if i % 5 == 4: print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 topic 의 단어생성확률은 아래의 함수로 가져올 수 있습니다. LdaModel.expElogbeta 에도 비슷한 정보가 포함되어 있지만, 정확한 정보는 아래의 함수를 이용해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4760)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.expElogbeta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_term_prob(lda_model):\n",
    "    topic_term_freqs = lda_model.state.get_lambda()\n",
    "    topic_term_prob = topic_term_freqs / topic_term_freqs.sum(axis=1)[:, None]\n",
    "    return topic_term_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = get_topic_term_prob(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 20)\n"
     ]
    }
   ],
   "source": [
    "topn = 20\n",
    "important_terms = beta.argsort(axis=1)[:,-topn:]\n",
    "print(important_terms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 print_topic() 함수의 결과는 아래의 과정을 거쳐 출력된 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "topic #0 : 단지 아파트 이용 예정 해수 분양 지역 환경 위치 골프 공급 구성 조성 전자신문 멜로디 제공 주거 게스트 가능 지하\n",
      "topic #1 : 사진 제품 선보 패션 브랜드 기사 화보 다양 피부 세번째 함께 인체 공개 판매 제공 24 가을 아이 음식 스포츠\n",
      "topic #2 : 웃음 클린턴 있습니다 트럼프 미국 혼술 후보 앵커 방송 라고 유지 그녀 사로 대선 토론 공명 선거 모습 지지 시청자들\n",
      "topic #3 : 기업 기술 지원 사업 세계 창업 개발 글로벌 산업 한국 성장 경제 시장 투자 계획 연구 교육 전문 미래 분야\n",
      "topic #4 : 영화 공연 일간스포츠 개봉 고양이 작품 할머니 장애인 관객 배우 티켓 11월 사랑 3일 뮤지컬 진행 감독 예술 주인 코미디\n",
      "\n",
      "topic #5 : 개헌 우리 국민 헌법 사회 대한민국 대통령 지금 개정 체제 정책 정치 경제 국회 변화 있습니다 논의 1987년 미래 정부\n",
      "topic #6 : 제보 배포 라이브 기억 동영상 전주 11월 훈훈 자신 결정 기록 저자 법인세 수학 확인 9년 족보 16일 인물들 광복군\n",
      "topic #7 : 현대 인천 공장 필리핀 대사 이동 건조 러시아 옥녀 계약 24 가짜 평년 호위 순천 처방 함유 수주 중국 불법\n",
      "topic #8 : 예산 정부 대통령 국회 예산안 지원 편성 경제 시정연설 관련 내년도 연설 확대 박근혜 국민 문화 내년 여야 야당 심사\n",
      "topic #9 : 행사 지난 참여 제공 000원 진행 함께 현지 부산 전국 마을 개최 주말 지역 광주 이번 학생들 저녁 아스트로 파일\n",
      "\n",
      "topic #10 : 뉴스 저작권자 카페 웹툰 02 이용 인터넷 01 24 이용자 검색 밴드 포털 독자 마이데일리 시사 블로그 실시간 이메일 이용자들\n",
      "topic #11 : 문화 체험 지역 제공 진행 여행 운영 할인 마련 다양 이번 참여 실시 예정 주민 있도록 보험 함께 직접 프로그램\n",
      "topic #12 : 총장 청춘 충남 대학 한국 세종역 신설 한의사 아시아 세종 합의 의장 회장 충북도 중앙 상단 원유 뉴스1 지역 지난\n",
      "topic #13 : 남자 여자 감독 수애 왕소 황제 신인 르노삼성 영화 고객 이준기 커피 스틸 김영광 우리 24 고난길 우리집 광종 제공\n",
      "topic #14 : 24 오후 독도 참석 인도네시아 제보 전남 연습 포토타임 전파 영화제 종로구 오전 이정 저작권자 세월호 2016 25일 한국 추모\n",
      "\n",
      "topic #15 : 공시 결제 카드 신한 조선 계약 카카오톡 인턴 미흡 제품으 24 내용 점검 이용 기재 카카오 16 플레이 기업 안희정\n",
      "topic #16 : 뉴스1 24 마약 라디오 선고 판결 대표 재판 두테르테 대통령 필리핀 기소 징역 항소 혐의 광진구 1심 반기문 크기 여성들\n",
      "topic #17 : 치료 환자 경우 건강 사용 여성 이상 연구 발생 병원 결과 수술 임신 필요 때문 강태 관리 진단 치매 위험\n",
      "topic #18 : 24 국회 2016 한남동 여의도 시정연설 박근혜 대통령 예산안 오전 2017년도 본회의장 관련 대세 참석 정성 새누리당 이정현 의장 인사\n",
      "topic #19 : 조사 사실 경찰 것으 혐의 확인 검찰 사건 수사 청와대 최씨 지난 공정위 당시 관련 의혹 2014년 보도 구속 24\n",
      "\n",
      "topic #20 : 아시아경제 인스타그램 수상 24 기사 선정 2016 오후 경기 세계 모델 모모 진행 시즌 우승 슈퍼 대회 배우 부문 선수\n",
      "topic #21 : 대구 제주 요리 아들 제주도 아내 학생들 바다 서울시 이별 오후 남편 알게 경찰 호텔 부지 것으 24 세종시 날씨\n",
      "topic #22 : 스포츠 의혹 난민 재단 최씨 검찰 독일 것으 한글 설립 수사 최순실 미르재단 프랑스 철거 이사장 대우 영국 운영 관련\n",
      "topic #23 : 무대 사랑 함께 노래 음악 모습 활동 팬들 마음 가수 사람 사진 지난 연인 소속사 우리 출연 아이 자신 관심\n",
      "topic #24 : 데뷔 배우 연기 드라마 방송 변신 라고 작품 자신 시청자들 연출 지난 이야기 작가 아빠 등장 가득 촬영 사람 사실\n",
      "\n",
      "topic #25 : 네이버 삼성 중국어선 삼성전자 조직 기술 선임 독립 이사회 24 아미카 부회장 개발자 주석 공개 중국 사업 회장 내년 음성\n",
      "topic #26 : 한국일보 경우 이들 서울신문 경찰 운영 것으 불법 위자료 지난해 이데일리 학교 종합 정씨 스크린 법원 도박 기준 특별 요구\n",
      "topic #27 : 부산 자전거 뿐만 파업 보컬 왕십리 오후 성동구 임대 2차 커플 24 도로 운행 철도노조 톡톡 지난 복귀 전일 전원\n",
      "topic #28 : 지진 교육 경기도 발생 수원 케미 오전 학교 피해 청소년 규모 지역 경기 경주 수원시 24 9시 임금 신세계 근로자\n",
      "topic #29 : 사진 영상 진영 잠시 24 터키 표정 박주 본부 연예인 이라크 한강 모술 최진 동상 시리아 조각 공감 페스티벌 2016\n",
      "\n",
      "topic #30 : 북한 외교 중국 우리 정부 주장 역도 미국 논란 대북 대변 입장 대화 관련 통일 의견 이성경 보도자료 회고록 적도\n",
      "topic #31 : 장관 대표 새누리당 대통령 회의 회고록 24 더불어민주당 관련 문재인 청와대 원내대표 주장 송민순 의혹 아프리카 발언 촉구 경제 진상규명\n",
      "topic #32 : 국내 해외 한국 수출 아이 프로 개발 루이 수산 사용 대표 있습니다 제품 전문 면역 바이어 효과 업체 시장 쇼핑\n",
      "topic #33 : 트와이스 공개 앨범 걸그룹 사랑 24 쇼케이스 매력 신곡 미니앨범 번째 블루스퀘어 선보 발매 캐릭터 오후 멤버들 1위 타이틀곡 발표\n",
      "topic #34 : 일본 서비스 온라인 엑소 모바일 고객 이벤트 유로 무릎 세계 롯데면세점 2위 1위 도쿄 제공 게재 3위 색다 월드 1박2일\n",
      "\n",
      "topic #35 : 2016 24 00 참석 23 기사 헤어질까 기념 오후 박규리 서준영 진행 중구 얌마 조성규 전경련 대화 언론시사회 속보 05\n",
      "topic #36 : 롯데 회장 방탄 그룹 눈빛 후문 나비 상장 한국 지분 지정 군인 호텔롯데 빌보드 지난 지배구조 각오 휘발유 롯데그룹 면세점\n",
      "topic #37 : 가격 부동산 것으 시장 정부 대출 올해 평균 지난해 기준 최근 수록 지난 경우 금리 때문 증가 우려 규제 이상\n",
      "topic #38 : 개헌 대통령 국회 헌법 임기 논의 제안 추진 정부 대선 개정 국민 4년 청와대 개헌안 뉴스1 박근혜 선거 내년 중임제\n",
      "topic #39 : 개헌 대통령 대표 논의 국민 입장 박근혜 최순실 국회 제안 새누리당 이날 개헌론 시정연설 이라고 정권 블랙홀 의혹 환영 24\n",
      "\n",
      "topic #40 : 게임 라인 화면 청취 블랙 동물 색상 조준 일들 모니터 여사 영상 부드 이영란 출시 헨리 배우들 24 움직임 제공\n",
      "topic #41 : 상승 주가 투자 거래 매수 기록 하락 24 대비 농협 발표 배용준 현재 최근 달러 유지 박수 외국인 코스 분석\n",
      "topic #42 : 스타일 코트 헤드 스포츠 노선 셰프 경리 직장인 특급 착용 대한항공 메르 제공 인천 벤츠 신규 운항 엔진 디자인 주행\n",
      "topic #43 : 방송 뉴미디어 사진 공개 배우 미디어 구르미 연설문 출연 자신 모습 드라마 한그 한편 한예슬 애정 24 송지효 김유정 소식\n",
      "topic #44 : 고호 방송 예능 사고 모습 23일 유지 복면가왕 이날 출연 버스 시청률 이동 현장 것으 노컷뉴스 기분 우비소녀 사진 양복점\n",
      "\n",
      "topic #45 : 시장 것으 리포트 판매 매출 지난해 출시 국내 올해 증가 전망 업계 중국 실적 판매량 3분기 성장 예상 소녀들 류준열\n",
      "topic #46 : 갤럭시 삼성전자 교환 갤럭시노트7 프로그램 노트7 사용 지원 미소 출시 고객 구입 추가 내년 할부금 24 가입 운영 구매 기존\n",
      "topic #47 : 가을 경남 저축 금융 24 강원 블락비 안전 기온 강원도 핀테크 이노 서현진 유연석 낭만 아침 단풍 전국 동해 시설\n",
      "topic #48 : 비정상 미국 증상 궁금증 유엔 이목 장식 대화 압박 제재 오른쪽 배신 국화 부상 노동자 치료제 24 정부 적응 접촉\n",
      "topic #49 : 때문 생각 하지 문제 지금 상황 사람 자신 라고 대표 정도 부분 그런데 그렇게 필요 질문 경우 사실 얘기 가장\n"
     ]
    }
   ],
   "source": [
    "for topic_idx in range(beta.shape[0]):\n",
    "    if topic_idx % 5 == 0:\n",
    "        print()\n",
    "    \n",
    "    term_indices = important_terms[topic_idx,:].reshape(-1)\n",
    "    terms = reversed([idx2vocab[idx] for idx in term_indices])\n",
    "\n",
    "    print('topic #{} : {}'.format(topic_idx, ' '.join(terms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim에서 단어의 topic vector를 직접적으로 찾아주는 함수가 구현되어 있지 않습니다. 하지만 LdaModel[bow_model]을 넣으면 bow_model에 대한 topic vector를 출력해줍니다. bow_model에 단어 한 개를 넣으면 해당 단어의 topic vector를 알 수 있습니다. \n",
    "\n",
    "bow_model은 [(term id, weight), (term id, weight), ... ] 형식입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(35, 0.51)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel[[(0,1)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어는 Vectorizer.vocabulary\\_를 이용하거나 Dictionary를 이용할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(words):\n",
    "    bow = [(vectorizer.vocabulary_.get(w, -1), f) for w,f in words]\n",
    "    bow = [(w, t) for w, t in bow if w != -1]\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 0.51)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_model = encode([('트와이스', 1)])\n",
    "ldamodel[bow_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_document_topics() 함수를 이용하면 minimum_probability, minimum_phi_value를 조절하며 topic vector를 만들 수 있습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(33, 0.51)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_model = encode([('트와이스', 1)])\n",
    "ldamodel.get_document_topics(bow_model,  minimum_probability=0.01, minimum_phi_value=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, 0.51)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_model = encode([('무대', 1)])\n",
    "ldamodel.get_document_topics(bow_model,  minimum_probability=0.01, minimum_phi_value=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 문서에 대해서도 topic probability vector 를 inferring 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23, 0.34), (33, 0.34)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_model = encode([('트와이스', 1), ('무대', 1)])\n",
    "ldamodel.get_document_topics(bow_model, minimum_probability=0.01, minimum_phi_value=0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
