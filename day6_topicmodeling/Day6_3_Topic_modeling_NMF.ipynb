{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 만들어 둔, 문서 - 단어 행렬을 이용하여 NMF 를 이용한 토픽모델링을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../../data/corpus_10days/models/params_keywords', 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "\n",
    "x = params['x']\n",
    "vocab2idx = params['word2index']\n",
    "idx2vocab = params['index2word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300,91 개의 문서가 9,774 개의 단어로 표현되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30091, 9774)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i7-5820 기준 7 분의 학습 시간이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovit/anaconda2/envs/scrapper/lib/python3.6/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator NMF from version 0.19.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "if False:\n",
    "    \n",
    "    nmf = NMF(n_components=100)\n",
    "    y = nmf.fit_transform(x)\n",
    "    \n",
    "    with open('./2016-10-20-nmf.pkl', 'wb') as f:\n",
    "        pickle.dump(nmf, f)\n",
    "    with open('./2016-10-20-nmf_y.pkl', 'wb') as f:\n",
    "        pickle.dump(y, f)\n",
    "\n",
    "else:\n",
    "    \n",
    "    with open('./2016-10-20-nmf.pkl', 'rb') as f:\n",
    "        nmf = pickle.load(f)\n",
    "    with open('./2016-10-20-nmf_y.pkl', 'rb') as f:\n",
    "        y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9,774 차원으로 표현되는 문서가 100 차원으로 표현됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30091, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9,774 개의 단어 역시 100 차원으로 표현됩니다. SVD 처럼 components_ 에 정보가 저장되어 있습니다.\n",
    "\n",
    "이를 반대로 해석하면 100 개의 components 들이 각각 9,774 개의 term weight vector 로 표현된 것과 같습니다. 30091 개의 문서를 표현할 수 있는 100 개의 components 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9774)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "안전하게 components vector 를 복사해둡니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "components = nmf.components_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 component 에 대하여 weight 의 크기가 큰 순서대로 component keyword 를 선택할 수 있습니다. 일종의 labeling 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('불독', 15.720180055497874),\n",
       " ('데뷔', 7.436744476677633),\n",
       " ('걸그룹', 6.392035159858045),\n",
       " ('쇼케이스', 4.529443114555796),\n",
       " ('키미', 4.244292664767637),\n",
       " ('형은', 4.037779583606048),\n",
       " ('무대', 3.4491967317880823),\n",
       " ('소라', 3.3814349219963007),\n",
       " ('롤링', 3.2202080370965027),\n",
       " ('세이', 3.08934217032449)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_important_terms(component, topn=10, decode=True):\n",
    "    \n",
    "    # sort by weight in decreasing order\n",
    "    terms = component.argsort()[::-1]\n",
    "    \n",
    "    # select top n terms\n",
    "    if topn > 0:\n",
    "        terms = terms[:topn]\n",
    "\n",
    "    # form of [(idx, weight), ... ]\n",
    "    weights = component[terms]\n",
    "    term_and_weights = [(t,w) for t,w in zip(terms, weights)]\n",
    "    \n",
    "    # decode\n",
    "    if decode:\n",
    "        term_and_weights = [(idx2vocab[t], w) for t,w in term_and_weights]\n",
    "    \n",
    "    return term_and_weights\n",
    "\n",
    "most_important_terms(components[71])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 단어에 대하여 weight 가 큰 components 를 찾을 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_relavant_components(term, n_components=5, n_terms=20, decode=True):\n",
    "    term_idx = vocab2idx.get(term, -1)\n",
    "    if term_idx < 0:\n",
    "        return []\n",
    "\n",
    "    # copy components for safety\n",
    "    components = nmf.components_.copy()\n",
    "    \n",
    "    # slice columns of term and sort in decreasing order\n",
    "    relavant_idx = components[:,term_idx].argsort()[::-1]\n",
    "    \n",
    "    # select top n_components\n",
    "    if n_components > 0:\n",
    "        relavant_component_idx = relavant_idx[:n_components]\n",
    "\n",
    "    # select important terms for each component\n",
    "    relevant_components = []\n",
    "    for component_idx in relavant_component_idx:\n",
    "        # idx, score, terms\n",
    "        relevant_components.append(\n",
    "            (component_idx,\n",
    "             components[component_idx, term_idx],\n",
    "             most_important_terms(components[component_idx], n_terms, decode))\n",
    "        )\n",
    "    \n",
    "    return relevant_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'아이오아이'라는 단어의 weight 가 큰 components 들의 top weighted words 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component#71 : 불독 데뷔 걸그룹 쇼케이스 키미 형은 무대 소라 롤링 세이 오전 마포구 매력 101 멤버들 싱글 프로듀스 강렬 20일 표현\n",
      "component#11 : 공개 모습 화보 캔디 공유 캐릭터 도깨비 매력 선보 영상 마음 촬영 한편 메이크업 소리 조안 장근석 특히 변신 기대감\n",
      "component#60 : 기록 1위 트와이스 스트리밍 방탄소년단 차트 누적 발표 뮤직비디오 올해 가온차트 2016년 데뷔 유튜브 최고 미니앨범 조회수 차지 1주년 부문\n",
      "component#23 : 방송 출연 이날 프로그램 전현무 무대 예능 웃음 오후 시청률 지상파 노래 김지민 아프리카 광고 라고 샤이니 한편 20일 이에\n",
      "component#79 : 신화 앨범 발매 13집 팬들 정규 컴백 활동 11월 그룹 이번 공개 콘서트 데뷔 예정 멤버들 기대 곡들 19년 아이돌\n"
     ]
    }
   ],
   "source": [
    "for component_idx, relavant_score, important_terms in most_relavant_components('아이오아이'):\n",
    "    terms = [term for term, _ in important_terms]\n",
    "    print('component#{} : {}'.format(\n",
    "        component_idx,' '.join(terms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'트럼프'라는 단어의 weight 가 큰 components 들의 top weighted words 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component#1 : 트럼프 클린턴 토론 대선 후보 힐러리 주장 공화당 미국 3차 선거 민주당 도널드 대선후보 결과 라스베이거스 지지 이날 이라고 발언\n",
      "component#39 : 후보 지금 힐러리 미국 말씀 트럼프 있습니다 이런 때문 제가 우리 사실 클린턴 생각합니다 경제 생각 발언 국가 대통령 도널드\n",
      "component#57 : 여성 남성 여성들 남성들 혐오 자신 사회 임신 남자 표현 운동 말하는 이상 낙태 차별 사건 비율 생각 여자 군대\n",
      "component#14 : 중국 필리핀 두테르테 양국 남중국해 베이징 주석 정상회담 분쟁 협력 영유권 시진핑 방문 경제 투자 성장률 갈등 수출 로드 확대\n",
      "component#42 : 영화 감독 개봉 작품 배우 연기 걷기왕 이야기 관객 관객들 제작 심은경 흥행 만복 주연 출연 배우들 스크린 럭키 영화제\n"
     ]
    }
   ],
   "source": [
    "for component_idx, relavant_score, important_terms in most_relavant_components('트럼프'):\n",
    "    terms = [term for term, _ in important_terms]\n",
    "    print('component#{} : {}'.format(\n",
    "        component_idx,' '.join(terms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "NMF 는 SVD 와 활용하는 방법은 비슷합니다. component 를 학습하는 방식이 다릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
